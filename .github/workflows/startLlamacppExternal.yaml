name: startLlamacppExternal

on:
  workflow_dispatch: # This allows the workflow to be triggered manually

permissions:
  contents: read

jobs:
  xtts_v2:
    runs-on: ubuntu-latest
    environment: LLama  # This specifies that the secret is from the "LLama" environment

    steps:
    - uses: actions/checkout@v3

    - name: Debugging the NGROK_SECRET
      run: |
        echo "Checking if NGROK_SECRET is available"
        echo "NGROK_SECRET: $NGROK_SECRET"  # This will print the secret value (ensure it's correctly passed)
        if [ -z "$NGROK_SECRET" ]; then
          echo "NGROK_SECRET is not set or empty!"  # Print a message if the secret is empty or not passed
          exit 1  # Exit the workflow if the secret is missing
        fi
      env:
        NGROK_SECRET: ${{ secrets.NGROK_SECRET }}  # Pass secret to this step for debugging

    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: 'pip'
      env:
        NGROK_SECRET: ${{ secrets.NGROK_SECRET }}  # Pass secret to this step

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai
      env:
        NGROK_SECRET: ${{ secrets.NGROK_SECRET }}  # Pass secret to this step

    - name: Getting the files
      run: |
        curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null
        echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | sudo tee /etc/apt/sources.list.d/ngrok.list
        sudo apt update -y
        sudo apt install ngrok -y
        
        # Ensure ngrok is configured with the auth token from GitHub secret
        echo "Configuring ngrok with authtoken"
        echo "${{ secrets.NGROK_SECRET }}" | ngrok config add-authtoken
        
        wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
        mv Phi-3-mini-4k-instruct-q4.gguf /tmp/llama-model.gguf
        docker pull ghcr.io/abetlen/llama-cpp-python:v0.2.79
      env:
        NGROK_SECRET: ${{ secrets.NGROK_SECRET }}  # Pass secret to this step

    - name: Running Server
      run: |
        docker run -d  -p 8000:8000 -v /tmp:/models -e MODEL=/models/llama-model.gguf ghcr.io/abetlen/llama-cpp-python:v0.2.79
        # Explicitly use the ngrok binary to expose the local server
        echo "Starting ngrok with token: $NGROK_SECRET"
        sudo nohup ngrok http 8000 > /tmp/ngrok.log 2>&1 &
        # Wait for ngrok to establish the tunnel
        sleep 5
        cat /tmp/ngrok.log  # Print ngrok logs to see if there is an error
      env:
        NGROK_SECRET: ${{ secrets.NGROK_SECRET }}  # Pass secret to this step
